{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando librearias\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "from Resources.mlTracker import *\n",
    "from Python.Style.styles import  *\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desactivando wrnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/training_shuffled.parquet\")\n",
    "training_us = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/undersampled_shuffled.parquet\")\n",
    "training_os = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/oversampled_shuffled.parquet\")\n",
    "testing1 = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/testing1.parquet\")\n",
    "testing2 = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/testing2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To delete columns\n",
    "total_columns = training.columns\n",
    "tdc = ['serialNumber','serialNumber_neighbor','fixed_path','FE-Comments','Conjunto','PSNumber']\n",
    "training_cols = list(filter(lambda x: x not in tdc, total_columns))\n",
    "#Seleccionando columnas\n",
    "training = training[training_cols]\n",
    "training_us = training_us[training_cols]\n",
    "training_os = training_os[training_cols]\n",
    "testing1 = testing1[training_cols]\n",
    "testing2 = testing2[training_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separando etiquetas\n",
    "#Training\n",
    "y = training['Communicating']\n",
    "X = training.drop('Communicating', axis=1)\n",
    "#Training undersampled\n",
    "y_us = training_us['Communicating']\n",
    "X_us = training_us.drop('Communicating', axis=1)\n",
    "#Training oversampled\n",
    "y_os = training_os['Communicating']\n",
    "X_os = training_os.drop('Communicating', axis=1)\n",
    "#Test1\n",
    "y_testing_1 = testing1['Communicating']\n",
    "X_testing_1 = testing1.drop('Communicating', axis=1)\n",
    "#Test2\n",
    "y_testing_2 = testing2['Communicating']\n",
    "X_testing_2 = testing2.drop('Communicating', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seteando experimento\n",
    "experiment_name = \"ML\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parámetros de búsqueda\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', 0.3],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 0.01],\n",
    "    'min_samples_leaf': [1, 2, 4, 0.01],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "#Métricas de interés\n",
    "scoring = {'Accuracy': make_scorer(accuracy_score), 'F1': make_scorer(f1_score, average='macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo y GridSearchCV\n",
    "rf = RandomForestClassifier()\n",
    "randomized_search = RandomizedSearchCV(\n",
    "                            estimator= rf,\n",
    "                            param_distributions= param_grid,\n",
    "                            n_iter= 10,\n",
    "                            cv= 3,\n",
    "                            scoring= scoring,\n",
    "                            refit= 'F1',\n",
    "                            return_train_score= False,\n",
    "                            n_jobs= 4,\n",
    "                            verbose= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando hiperparámetros para Random Forest con df Training completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustando para training\n",
    "with mlflow.start_run(run_name=\"RF-Training\"):\n",
    "    rf1 = randomized_search.fit(X, y)\n",
    "    #Mejor modelo\n",
    "    best_model = rf1.best_estimator_\n",
    "    #Guardando resultados\n",
    "    mlflow.log_params(rf1.best_params_)\n",
    "    mlflow.log_metrics({\"best_score\":rf1.best_score_})\n",
    "    mlflow.sklearn.log_model(best_model,\"best_model\")\n",
    "    mlflow.sklearn.log_model(rf1, \"RandomizedSearchCV\")\n",
    "    #Guardando diccionario de resultados\n",
    "    results = rf1.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        results.to_parquet(temp.name)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, rf1.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, rf1.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, rf1.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, rf1.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error anterior es independiente del entrenamiento, fue causado debido a los tipos de datos en una columna particular. Por tal motivo se almacenará en el run que fue interrumpido, sin volver a entrenar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = get_run_id(experiment_name, \"RF-Training\")\n",
    "#Guardando resultados pendientes\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    #Guardando df con\n",
    "    results = rf1.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        try:\n",
    "            results.to_parquet(temp.name)\n",
    "        except:\n",
    "            results = results.astype(str)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, rf1.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, rf1.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, rf1.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, rf1.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando hiperparámetros para Random Forest con df Training balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=0.01, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.801) F1: (test=0.801) total time=  10.9s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=0.01, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.798) F1: (test=0.798) total time=  10.4s\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=0.01, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.794) F1: (test=0.793) total time=  10.5s\n",
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; Accuracy: (test=0.973) F1: (test=0.973) total time=  10.8s\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; Accuracy: (test=0.981) F1: (test=0.981) total time=  46.3s\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; Accuracy: (test=0.982) F1: (test=0.982) total time=  47.8s\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; Accuracy: (test=0.981) F1: (test=0.981) total time=  48.0s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; Accuracy: (test=0.976) F1: (test=0.976) total time=  10.6s\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; Accuracy: (test=0.972) F1: (test=0.972) total time=  10.6s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=200; Accuracy: (test=0.982) F1: (test=0.982) total time= 1.3min\n",
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=200; Accuracy: (test=0.980) F1: (test=0.980) total time= 1.3min\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=200; Accuracy: (test=0.980) F1: (test=0.980) total time= 1.3min\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=0.01, n_estimators=100; Accuracy: (test=0.913) F1: (test=0.913) total time=  43.6s\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=0.01, n_estimators=100; Accuracy: (test=0.916) F1: (test=0.916) total time=  42.7s\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=0.01, n_estimators=100; Accuracy: (test=0.907) F1: (test=0.907) total time=  42.9s\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=4, min_samples_split=2, n_estimators=400; Accuracy: (test=0.983) F1: (test=0.983) total time= 3.5min\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=4, min_samples_split=2, n_estimators=400; Accuracy: (test=0.985) F1: (test=0.985) total time= 3.4min\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=4, min_samples_split=2, n_estimators=400; Accuracy: (test=0.985) F1: (test=0.985) total time= 3.5min\n",
      "[CV 1/3] END bootstrap=True, criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.885) F1: (test=0.885) total time=  17.0s\n",
      "[CV 2/3] END bootstrap=True, criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.879) F1: (test=0.879) total time=  16.8s\n",
      "[CV 3/3] END bootstrap=True, criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.877) F1: (test=0.877) total time=  17.0s\n",
      "[CV 1/3] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; Accuracy: (test=0.976) F1: (test=0.976) total time=  35.0s\n",
      "[CV 2/3] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; Accuracy: (test=0.978) F1: (test=0.978) total time=  35.3s\n",
      "[CV 3/3] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; Accuracy: (test=0.976) F1: (test=0.976) total time=  35.6s\n",
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=100; Accuracy: (test=0.978) F1: (test=0.978) total time=  38.4s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=100; Accuracy: (test=0.980) F1: (test=0.980) total time=  38.4s\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=100; Accuracy: (test=0.977) F1: (test=0.977) total time=  38.7s\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=0.01, n_estimators=1000; Accuracy: (test=0.877) F1: (test=0.877) total time= 6.0min\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=0.01, n_estimators=1000; Accuracy: (test=0.880) F1: (test=0.880) total time= 6.0min\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=0.01, n_estimators=1000; Accuracy: (test=0.875) F1: (test=0.875) total time= 6.0min\n"
     ]
    }
   ],
   "source": [
    "#Ajustando para training undersampled\n",
    "with mlflow.start_run(run_name=\"RF-Training-Undersampled\"):\n",
    "    rf2 = randomized_search.fit(X_us, y_us)\n",
    "    #Mejor modelo\n",
    "    best_model = rf2.best_estimator_\n",
    "    #Guardando resultados\n",
    "    mlflow.log_params(rf2.best_params_)\n",
    "    mlflow.log_metrics({\"best_score\":rf2.best_score_})\n",
    "    mlflow.sklearn.log_model(best_model,\"best_model\")\n",
    "    mlflow.sklearn.log_model(rf2, \"RandomizedSearchCV\")\n",
    "    #Guardando diccionario de resultados\n",
    "    results = rf2.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        try:\n",
    "            results.to_parquet(temp.name)\n",
    "        except:\n",
    "            results = results.astype(str)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, rf2.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, rf2.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, rf2.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, rf2.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando hiperparámetros para Random Forest con df Training balanceado con oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo y GridSearchCV\n",
    "rf = RandomForestClassifier()\n",
    "randomized_search_3 = RandomizedSearchCV(\n",
    "                            estimator= rf,\n",
    "                            param_distributions= param_grid,\n",
    "                            n_iter= 5,\n",
    "                            cv= 3,\n",
    "                            scoring= scoring,\n",
    "                            refit= 'F1',\n",
    "                            return_train_score= False,\n",
    "                            n_jobs= 4,\n",
    "                            verbose= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; Accuracy: (test=0.923) F1: (test=0.923) total time=32.2min\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=1, min_samples_split=0.01, n_estimators=400; Accuracy: (test=0.919) F1: (test=0.919) total time=50.6min\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=1, min_samples_split=0.01, n_estimators=400; Accuracy: (test=0.918) F1: (test=0.918) total time=50.7min\n",
      "[CV 1/3] END bootstrap=True, criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400; Accuracy: (test=nan) F1: (test=nan) total time=   0.6s\n",
      "[CV 2/3] END bootstrap=True, criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400; Accuracy: (test=nan) F1: (test=nan) total time=   0.6s\n",
      "[CV 3/3] END bootstrap=True, criterion=entropy, max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=400; Accuracy: (test=nan) F1: (test=nan) total time=   0.5s\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=1, min_samples_split=0.01, n_estimators=400; Accuracy: (test=0.918) F1: (test=0.918) total time=50.8min\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; Accuracy: (test=0.923) F1: (test=0.923) total time=32.0min\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; Accuracy: (test=0.923) F1: (test=0.923) total time=33.4min\n",
      "[CV 2/3] END bootstrap=True, criterion=entropy, max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=5, n_estimators=400; Accuracy: (test=0.999) F1: (test=0.999) total time=64.4min\n",
      "[CV 1/3] END bootstrap=True, criterion=entropy, max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=5, n_estimators=400; Accuracy: (test=1.000) F1: (test=1.000) total time=64.8min\n",
      "[CV 1/3] END bootstrap=True, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=4, min_samples_split=10, n_estimators=200; Accuracy: (test=0.999) F1: (test=0.999) total time=70.2min\n",
      "[CV 3/3] END bootstrap=True, criterion=entropy, max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=5, n_estimators=400; Accuracy: (test=0.999) F1: (test=0.999) total time=95.3min\n",
      "[CV 2/3] END bootstrap=True, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=4, min_samples_split=10, n_estimators=200; Accuracy: (test=0.999) F1: (test=0.999) total time=54.0min\n",
      "[CV 3/3] END bootstrap=True, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=4, min_samples_split=10, n_estimators=200; Accuracy: (test=0.999) F1: (test=0.999) total time=53.9min\n"
     ]
    }
   ],
   "source": [
    "#Ajustando para training undersampled\n",
    "with mlflow.start_run(run_name=\"RF-Training-Oversampled\"):\n",
    "    #Guardando información de experimento\n",
    "    mlflow.log_param(\"data\",\"oversampled\")\n",
    "    mlflow.log_param(\"model\",\"RandomForest\")\n",
    "    mlflow.log_param(\"dist_n_estimators\",\"100, 200, 300, 400, 500, 1000\")\n",
    "    mlflow.log_param(\"dist_max_features\",\"auto, sqrt, log2, 0.3\")\n",
    "    mlflow.log_param(\"dist_max_depth\",\"None, 10, 20, 30, 40, 50\")\n",
    "    mlflow.log_param(\"dist_min_samples_split\",\"2, 5, 10, 0.01\")\n",
    "    mlflow.log_param(\"dist_min_samples_leaf\",\"1, 2, 4, 0.01\")\n",
    "    mlflow.log_param(\"dist_bootstrap\",\"True, False\")\n",
    "    mlflow.log_param(\"dist_criterion\",\"gini, entropy\")\n",
    "    mlflow.log_param(\"n_iter\",5)\n",
    "    rf3 = randomized_search_3.fit(X_os, y_os)\n",
    "    #Mejor modelo\n",
    "    best_model = rf3.best_estimator_\n",
    "    #Guardando resultados\n",
    "    mlflow.log_params(rf3.best_params_)\n",
    "    mlflow.log_metrics({\"best_score\":rf3.best_score_})\n",
    "    mlflow.sklearn.log_model(best_model,\"best_model\")\n",
    "    mlflow.sklearn.log_model(rf3, \"RandomizedSearchCV\")\n",
    "    #Guardando diccionario de resultados\n",
    "    results = rf3.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        try:\n",
    "            results.to_parquet(temp.name)\n",
    "        except:\n",
    "            results = results.astype(str)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, rf3.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, rf3.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, rf3.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, rf3.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
