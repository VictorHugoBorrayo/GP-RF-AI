{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando librearias\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "from sklearn.svm import SVC\n",
    "from Resources.mlTracker import *\n",
    "from Python.Style.styles import  *\n",
    "from scipy.stats import expon, randint\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desactivando wrnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/training_shuffled.parquet\")\n",
    "training_us = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/undersampled_shuffled.parquet\")\n",
    "testing1 = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/testing1.parquet\")\n",
    "testing2 = pd.read_parquet(\"../../Data/DataMart/Views/TrainingViews/testing2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To delete columns\n",
    "total_columns = training.columns\n",
    "tdc = ['serialNumber','serialNumber_neighbor','fixed_path','FE-Comments','Conjunto','PSNumber']\n",
    "training_cols = list(filter(lambda x: x not in tdc, total_columns))\n",
    "#Seleccionando columnas\n",
    "training = training[training_cols]\n",
    "training_us = training_us[training_cols]\n",
    "testing1 = testing1[training_cols]\n",
    "testing2 = testing2[training_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separando etiquetas\n",
    "#Training\n",
    "y = training['Communicating']\n",
    "X = training.drop('Communicating', axis=1)\n",
    "#Training undersampled\n",
    "y_us = training_us['Communicating']\n",
    "X_us = training_us.drop('Communicating', axis=1)\n",
    "#Test1\n",
    "y_testing_1 = testing1['Communicating']\n",
    "X_testing_1 = testing1.drop('Communicating', axis=1)\n",
    "#Test2\n",
    "y_testing_2 = testing2['Communicating']\n",
    "X_testing_2 = testing2.drop('Communicating', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seteando experimento\n",
    "experiment_name = \"ML\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parámetros de búsqueda\n",
    "param_dist = {\n",
    "    'C': expon(scale=100),\n",
    "    'gamma': expon(scale=.1),\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "#Métricas de interés\n",
    "scoring = {'Accuracy': make_scorer(accuracy_score), 'F1': make_scorer(f1_score, average='macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo y GridSearchCV\n",
    "svm = SVC()\n",
    "randomized_search = RandomizedSearchCV(\n",
    "                            estimator= svm,\n",
    "                            param_distributions= param_dist,\n",
    "                            n_iter= 5,\n",
    "                            cv= 3,\n",
    "                            scoring= scoring,\n",
    "                            refit= 'F1',\n",
    "                            return_train_score= False,\n",
    "                            n_jobs= 4,\n",
    "                            verbose= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando hiperparámetros para Support Vector Machine con conjunto de datos con submuestreo de la clase mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV 1/3] END C=51.05083011988179, class_weight=None, gamma=0.13386430541158195, kernel=rbf; Accuracy: (test=0.917) F1: (test=0.917) total time= 5.5min\n",
      "[CV 2/3] END C=51.05083011988179, class_weight=None, gamma=0.13386430541158195, kernel=rbf; Accuracy: (test=0.921) F1: (test=0.921) total time= 6.8min\n",
      "[CV 3/3] END C=51.05083011988179, class_weight=None, gamma=0.13386430541158195, kernel=rbf; Accuracy: (test=0.917) F1: (test=0.917) total time= 6.6min\n",
      "[CV 3/3] END C=47.455796263279495, class_weight=balanced, gamma=0.343890224078124, kernel=linear; Accuracy: (test=0.767) F1: (test=0.767) total time=29.8min\n",
      "[CV 2/3] END C=47.455796263279495, class_weight=balanced, gamma=0.343890224078124, kernel=linear; Accuracy: (test=0.762) F1: (test=0.762) total time=29.8min\n",
      "[CV 1/3] END C=47.455796263279495, class_weight=balanced, gamma=0.343890224078124, kernel=linear; Accuracy: (test=0.767) F1: (test=0.767) total time=30.0min\n",
      "[CV 1/3] END C=122.51237755741045, class_weight=None, gamma=0.010379951247369325, kernel=rbf; Accuracy: (test=0.863) F1: (test=0.863) total time= 3.2min\n",
      "[CV 2/3] END C=122.51237755741045, class_weight=None, gamma=0.010379951247369325, kernel=rbf; Accuracy: (test=0.863) F1: (test=0.863) total time=18.3min\n",
      "[CV 3/3] END C=122.51237755741045, class_weight=None, gamma=0.010379951247369325, kernel=rbf; Accuracy: (test=0.859) F1: (test=0.859) total time=13.0min\n",
      "[CV 1/3] END C=25.376142954111145, class_weight=None, gamma=0.11558322004294062, kernel=sigmoid; Accuracy: (test=0.573) F1: (test=0.573) total time= 2.1min\n",
      "[CV 2/3] END C=25.376142954111145, class_weight=None, gamma=0.11558322004294062, kernel=sigmoid; Accuracy: (test=0.565) F1: (test=0.565) total time= 2.1min\n",
      "[CV 1/3] END C=140.8896912843631, class_weight=balanced, gamma=0.14488449205803444, kernel=poly; Accuracy: (test=0.912) F1: (test=0.912) total time=50.3min\n",
      "[CV 3/3] END C=25.376142954111145, class_weight=None, gamma=0.11558322004294062, kernel=sigmoid; Accuracy: (test=0.568) F1: (test=0.568) total time= 2.0min\n",
      "[CV 3/3] END C=140.8896912843631, class_weight=balanced, gamma=0.14488449205803444, kernel=poly; Accuracy: (test=0.911) F1: (test=0.911) total time=49.2min\n",
      "[CV 2/3] END C=140.8896912843631, class_weight=balanced, gamma=0.14488449205803444, kernel=poly; Accuracy: (test=0.914) F1: (test=0.914) total time=49.2min\n"
     ]
    }
   ],
   "source": [
    "#Ajustando para training\n",
    "with mlflow.start_run(run_name=\"SVM-Undersampled-2\"):\n",
    "    svm1 = randomized_search.fit(X_us, y_us)\n",
    "    #Mejor modelo\n",
    "    best_model = svm1.best_estimator_\n",
    "    #Guardando resultados\n",
    "    mlflow.log_params(svm1.best_params_)\n",
    "    mlflow.log_metrics({\"best_score\":svm1.best_score_})\n",
    "    mlflow.sklearn.log_model(best_model,\"best_model\")\n",
    "    mlflow.sklearn.log_model(svm1, \"RandomizedSearchCV\")\n",
    "    #Guardando diccionario de resultados\n",
    "    results = svm1.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        results.to_parquet(temp.name)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, svm1.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, svm1.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, svm1.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, svm1.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando hiperparámetros para Support Vector Machine con conjunto de datos con submuestreo de la clase mayoritaria - exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parámetros de búsqueda\n",
    "param_dist_3 = {\n",
    "    'C': expon(scale=10),\n",
    "    'gamma': expon(scale=.1),\n",
    "    'kernel': ['rbf', 'linear', 'poly'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "#Métricas de interés\n",
    "scoring = {'Accuracy': make_scorer(accuracy_score), 'F1': make_scorer(f1_score, average='macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo y GridSearchCV\n",
    "svm = SVC()\n",
    "randomized_search_3 = RandomizedSearchCV(\n",
    "                            estimator= svm,\n",
    "                            param_distributions= param_dist_3,\n",
    "                            n_iter= 10,\n",
    "                            cv= 3,\n",
    "                            scoring= scoring,\n",
    "                            refit= 'F1',\n",
    "                            return_train_score= False,\n",
    "                            n_jobs= 4,\n",
    "                            verbose= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustando para training\n",
    "with mlflow.start_run(run_name=\"SVM-Undersampled-3\"):\n",
    "    #Agregando información\n",
    "    mlflow.log_param(\"model\",\"SVM\")\n",
    "    mlflow.log_param(\"data\",\"Undersampled\")\n",
    "    mlflow.log_param(\"dist_C\",\"expon(scale=10)\")\n",
    "    mlflow.log_param(\"dist_gamma\",\"expon(scale=.1)\")\n",
    "    mlflow.log_param(\"dist_kernel\",\"rbf, linear, poly\")\n",
    "    mlflow.log_param(\"dist_class_weight\",\"balanced\")\n",
    "    mlflow.log_param(\"n_iter\",10)\n",
    "    svm3 = randomized_search_3.fit(X_us, y_us)\n",
    "    #Mejor modelo\n",
    "    best_model = svm3.best_estimator_\n",
    "    #Guardando resultados\n",
    "    mlflow.log_params(svm3.best_params_)\n",
    "    mlflow.log_metrics({\"best_score\":svm3.best_score_})\n",
    "    mlflow.sklearn.log_model(best_model,\"best_model\")\n",
    "    mlflow.sklearn.log_model(svm3, \"RandomizedSearchCV\")\n",
    "    #Guardando diccionario de resultados\n",
    "    results = svm3.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        results.to_parquet(temp.name)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, svm3.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, svm3.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, svm3.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, svm3.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9141182787887407"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883754934247245 0.574162833861682 0.9022238904006756 0.8673994446850243\n"
     ]
    }
   ],
   "source": [
    "#Calculando métricas con conjuntos de testeo}\n",
    "f1_test1 = f1_score(y_testing_1, svm3.predict(X_testing_1), average='macro')\n",
    "f1_test2 = f1_score(y_testing_2, svm3.predict(X_testing_2), average='macro')\n",
    "accuracy_test1 = accuracy_score(y_testing_1, svm3.predict(X_testing_1))\n",
    "accuracy_test2 = accuracy_score(y_testing_2, svm3.predict(X_testing_2))\n",
    "print(f1_test1, f1_test2, accuracy_test1, accuracy_test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error anterior es independiente del entrenamiento, fue causado debido a los tipos de datos en una columna particular. Por tal motivo se almacenará en el run que fue interrumpido, sin volver a entrenar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = get_run_id(experiment_name, \"RF-Training\")\n",
    "#Guardando resultados pendientes\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    #Guardando df con\n",
    "    results = rf1.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        try:\n",
    "            results.to_parquet(temp.name)\n",
    "        except:\n",
    "            results = results.astype(str)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, rf1.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, rf1.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, rf1.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, rf1.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando hiperparámetros para Random Forest con df Training balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=0.01, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.801) F1: (test=0.801) total time=  10.9s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=0.01, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.798) F1: (test=0.798) total time=  10.4s\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=10, max_features=sqrt, min_samples_leaf=0.01, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.794) F1: (test=0.793) total time=  10.5s\n",
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; Accuracy: (test=0.973) F1: (test=0.973) total time=  10.8s\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; Accuracy: (test=0.981) F1: (test=0.981) total time=  46.3s\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; Accuracy: (test=0.982) F1: (test=0.982) total time=  47.8s\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; Accuracy: (test=0.981) F1: (test=0.981) total time=  48.0s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; Accuracy: (test=0.976) F1: (test=0.976) total time=  10.6s\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; Accuracy: (test=0.972) F1: (test=0.972) total time=  10.6s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=200; Accuracy: (test=0.982) F1: (test=0.982) total time= 1.3min\n",
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=200; Accuracy: (test=0.980) F1: (test=0.980) total time= 1.3min\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=200; Accuracy: (test=0.980) F1: (test=0.980) total time= 1.3min\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=0.01, n_estimators=100; Accuracy: (test=0.913) F1: (test=0.913) total time=  43.6s\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=0.01, n_estimators=100; Accuracy: (test=0.916) F1: (test=0.916) total time=  42.7s\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=0.01, n_estimators=100; Accuracy: (test=0.907) F1: (test=0.907) total time=  42.9s\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=4, min_samples_split=2, n_estimators=400; Accuracy: (test=0.983) F1: (test=0.983) total time= 3.5min\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=4, min_samples_split=2, n_estimators=400; Accuracy: (test=0.985) F1: (test=0.985) total time= 3.4min\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=40, max_features=0.3, min_samples_leaf=4, min_samples_split=2, n_estimators=400; Accuracy: (test=0.985) F1: (test=0.985) total time= 3.5min\n",
      "[CV 1/3] END bootstrap=True, criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.885) F1: (test=0.885) total time=  17.0s\n",
      "[CV 2/3] END bootstrap=True, criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.879) F1: (test=0.879) total time=  16.8s\n",
      "[CV 3/3] END bootstrap=True, criterion=entropy, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=0.01, n_estimators=200; Accuracy: (test=0.877) F1: (test=0.877) total time=  17.0s\n",
      "[CV 1/3] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; Accuracy: (test=0.976) F1: (test=0.976) total time=  35.0s\n",
      "[CV 2/3] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; Accuracy: (test=0.978) F1: (test=0.978) total time=  35.3s\n",
      "[CV 3/3] END bootstrap=True, criterion=entropy, max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; Accuracy: (test=0.976) F1: (test=0.976) total time=  35.6s\n",
      "[CV 1/3] END bootstrap=True, criterion=gini, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=100; Accuracy: (test=0.978) F1: (test=0.978) total time=  38.4s\n",
      "[CV 2/3] END bootstrap=True, criterion=gini, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=100; Accuracy: (test=0.980) F1: (test=0.980) total time=  38.4s\n",
      "[CV 3/3] END bootstrap=True, criterion=gini, max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=100; Accuracy: (test=0.977) F1: (test=0.977) total time=  38.7s\n",
      "[CV 1/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=0.01, n_estimators=1000; Accuracy: (test=0.877) F1: (test=0.877) total time= 6.0min\n",
      "[CV 2/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=0.01, n_estimators=1000; Accuracy: (test=0.880) F1: (test=0.880) total time= 6.0min\n",
      "[CV 3/3] END bootstrap=False, criterion=entropy, max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=0.01, n_estimators=1000; Accuracy: (test=0.875) F1: (test=0.875) total time= 6.0min\n"
     ]
    }
   ],
   "source": [
    "#Ajustando para training undersampled\n",
    "with mlflow.start_run(run_name=\"RF-Training-Undersampled\"):\n",
    "    rf2 = randomized_search.fit(X_us, y_us)\n",
    "    #Mejor modelo\n",
    "    best_model = rf2.best_estimator_\n",
    "    #Guardando resultados\n",
    "    mlflow.log_params(rf2.best_params_)\n",
    "    mlflow.log_metrics({\"best_score\":rf2.best_score_})\n",
    "    mlflow.sklearn.log_model(best_model,\"best_model\")\n",
    "    mlflow.sklearn.log_model(rf2, \"RandomizedSearchCV\")\n",
    "    #Guardando diccionario de resultados\n",
    "    results = rf2.cv_results_\n",
    "    results = pd.DataFrame(results)\n",
    "    with tempfile.NamedTemporaryFile(mode = \"w+\", suffix = \".parquet\", delete= False) as temp:\n",
    "        try:\n",
    "            results.to_parquet(temp.name)\n",
    "        except:\n",
    "            results = results.astype(str)\n",
    "        mlflow.log_artifact(temp.name,\"results/results.parquet\")\n",
    "        temp_path = temp.name\n",
    "    #Almacenando métricas con conjunto de datos\n",
    "    f1_test1 = f1_score(y_testing_1, rf2.predict(X_testing_1), average='macro')\n",
    "    f1_test2 = f1_score(y_testing_2, rf2.predict(X_testing_2), average='macro')\n",
    "    accuracy_test1 = accuracy_score(y_testing_1, rf2.predict(X_testing_1))\n",
    "    accuracy_test2 = accuracy_score(y_testing_2, rf2.predict(X_testing_2))\n",
    "    mlflow.log_metrics({\"f1_test1\":f1_test1, \"f1_test2\":f1_test2, \"accuracy_test1\":accuracy_test1, \"accuracy_test2\":accuracy_test2})\n",
    "    os.remove(temp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
